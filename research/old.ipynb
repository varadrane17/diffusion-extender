{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# import spaces\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, TCDScheduler\n",
    "from diffusers.models.model_loading_utils import load_state_dict\n",
    "from gradio_imageslider import ImageSlider\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from src.controlnet_union import ControlNetModel_Union\n",
    "from src.pipeline_fill_sd_xl import StableDiffusionXLFillPipeline\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A mixture of fp16 and non-fp16 filenames will be loaded.\n",
      "Loaded fp16 filenames:\n",
      "[unet/diffusion_pytorch_model.fp16.safetensors, text_encoder_2/model.fp16.safetensors, vae/diffusion_pytorch_model.fp16.safetensors, text_encoder/model.fp16.safetensors]\n",
      "Loaded non-fp16 filenames:\n",
      "[unet/diffusion_pytorch_model-00002-of-00002.safetensors, unet/diffusion_pytorch_model-00001-of-00002.safetensors\n",
      "If this behavior is not expected, please check your folder structure.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  7.31it/s]\n"
     ]
    }
   ],
   "source": [
    "config_file = hf_hub_download(\n",
    "    \"xinsir/controlnet-union-sdxl-1.0\",\n",
    "    filename=\"config_promax.json\",\n",
    ")\n",
    "\n",
    "config = ControlNetModel_Union.load_config(config_file)\n",
    "controlnet_model = ControlNetModel_Union.from_config(config)\n",
    "model_file = hf_hub_download(\n",
    "    \"xinsir/controlnet-union-sdxl-1.0\",\n",
    "    filename=\"diffusion_pytorch_model_promax.safetensors\",\n",
    ")\n",
    "state_dict = load_state_dict(model_file)\n",
    "model, _, _, _, _ = ControlNetModel_Union._load_pretrained_model(\n",
    "    controlnet_model, state_dict, model_file, \"xinsir/controlnet-union-sdxl-1.0\"\n",
    ")\n",
    "model.to(device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe = StableDiffusionXLFillPipeline.from_pretrained(\n",
    "    \"SG161222/RealVisXL_V5.0_Lightning\",\n",
    "    torch_dtype=torch.float16,\n",
    "    vae=vae,\n",
    "    controlnet=model,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_expand(source_width, source_height, target_width, target_height, alignment):\n",
    "    \"\"\"Checks if the image can be expanded based on the alignment.\"\"\"\n",
    "    if alignment in (\"Left\", \"Right\") and source_width >= target_width:\n",
    "        return False\n",
    "    if alignment in (\"Top\", \"Bottom\") and source_height >= target_height:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "def base64_to_img(img_str: str) -> Image.Image:\n",
    "    return Image.open(BytesIO(base64.b64decode(img_str)))\n",
    "\n",
    "def img_to_base64(img: Image.Image) -> str:\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image_path, width, height, overlap_width, num_inference_steps, resize_option, custom_resize_size, prompt_input=None, alignment=\"Middle\"):\n",
    "    source = Image.open(image_path)\n",
    "    print(f\"Source Image : {source.size}\")\n",
    "    target_size = (width, height)\n",
    "    overlap = overlap_width\n",
    "\n",
    "    # Upscale if source is smaller than target in both dimensions\n",
    "    # if source.width < target_size[0] and source.height < target_size[1]:\n",
    "    #     scale_factor = min(target_size[0] / source.width, target_size[1] / source.height)\n",
    "    #     new_width = int(source.width * scale_factor)\n",
    "    #     new_height = int(source.height * scale_factor)\n",
    "    #     source = source.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    # if source.width > target_size[0] or source.height > target_size[1]:\n",
    "    #     scale_factor = min(target_size[0] / source.width, target_size[1] / source.height)\n",
    "    #     new_width = int(source.width * scale_factor)\n",
    "    #     new_height = int(source.height * scale_factor)\n",
    "    #     source = source.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    if resize_option == \"Full\":\n",
    "        resize_size = max(source.width, source.height)\n",
    "    elif resize_option == \"1/2\":\n",
    "        resize_size = max(source.width, source.height) // 2\n",
    "    elif resize_option == \"1/3\":\n",
    "        resize_size = max(source.width, source.height) // 3\n",
    "    elif resize_option == \"1/4\":\n",
    "        resize_size = max(source.width, source.height) // 4\n",
    "    else:  # Custom\n",
    "        resize_size = custom_resize_size\n",
    "    print(f\"Original Size : {source.size}\")\n",
    "    aspect_ratio = source.height / source.width\n",
    "    print(f\"Aspect Ratio : {aspect_ratio}\")\n",
    "    new_width = resize_size\n",
    "    print(f\"New Width : {new_width}\")\n",
    "    new_height = int(resize_size * aspect_ratio)\n",
    "    print(f\"New Height : {new_height}\")\n",
    "    source = source.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    # if not can_expand(source.width, source.height, target_size[0], target_size[1], alignment):\n",
    "    #     alignment = \"Middle\"\n",
    "\n",
    "    # Calculate margins based on alignment\n",
    "    # if alignment == \"Middle\":\n",
    "    #     margin_x = (target_size[0] - source.width) // 2\n",
    "    #     margin_y = (target_size[1] - source.height) // 2\n",
    "    # elif alignment == \"Left\":\n",
    "    #     margin_x = 0\n",
    "    #     margin_y = (target_size[1] - source.height) // 2\n",
    "    # elif alignment == \"Right\":\n",
    "    #     margin_x = target_size[0] - source.width\n",
    "    #     margin_y = (target_size[1] - source.height) // 2\n",
    "    # elif alignment == \"Top\":\n",
    "    #     margin_x = (target_size[0] - source.width) // 2\n",
    "    #     margin_y = 0\n",
    "    # elif alignment == \"Bottom\":\n",
    "    #     margin_x = (target_size[0] - source.width) // 2\n",
    "    #     margin_y = target_size[1] - source.height\n",
    "\n",
    "    margin_x = (target_size[0] - source.width) // 2\n",
    "    margin_y = (target_size[1] - source.height) // 2\n",
    "\n",
    "\n",
    "    background = Image.new('RGB', target_size, (255, 255, 255))\n",
    "    background.paste(source, (margin_x, margin_y))\n",
    "\n",
    "    mask = Image.new('L', target_size, 255)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # Adjust mask generation based on alignment\n",
    "    # if alignment == \"Middle\":\n",
    "    #     mask_draw.rectangle([\n",
    "    #         (margin_x + overlap, margin_y + overlap),\n",
    "    #         (margin_x + source.width - overlap, margin_y + source.height - overlap)\n",
    "    #     ], fill=0)\n",
    "    # elif alignment == \"Left\":\n",
    "    #     mask_draw.rectangle([\n",
    "    #         (margin_x, margin_y),\n",
    "    #         (margin_x + source.width - overlap, margin_y + source.height)\n",
    "    #     ], fill=0)\n",
    "    # elif alignment == \"Right\":\n",
    "    #     mask_draw.rectangle([\n",
    "    #         (margin_x + overlap, margin_y),\n",
    "    #         (margin_x + source.width, margin_y + source.height)\n",
    "    #     ], fill=0)\n",
    "    # elif alignment == \"Top\":\n",
    "    #     mask_draw.rectangle([\n",
    "    #         (margin_x, margin_y),\n",
    "    #         (margin_x + source.width, margin_y + source.height - overlap)\n",
    "    #     ], fill=0)\n",
    "    # elif alignment == \"Bottom\":\n",
    "    #     mask_draw.rectangle([\n",
    "    #         (margin_x, margin_y + overlap),\n",
    "    #         (margin_x + source.width, margin_y + source.height)\n",
    "    #     ], fill=0)\n",
    "\n",
    "    mask_draw.rectangle([\n",
    "        (margin_x + overlap, margin_y + overlap),\n",
    "        (margin_x + source.width - overlap, margin_y + source.height - overlap)\n",
    "    ], fill=0)\n",
    "\n",
    "    cnet_image = background.copy()\n",
    "    cnet_image.paste(0, (0, 0), mask)\n",
    "\n",
    "    final_prompt = f\"high quality, 4k\"\n",
    "\n",
    "#     import requests\n",
    "#     import json\n",
    "#     url = \"http://0.0.0.0:8000/generate-image/\"\n",
    "\n",
    "#     order_id = \"94r804t8t-e830r8r48t\"\n",
    "#     cnet_image.save(\"cnet_image_original.png\")\n",
    "#     c_net_base64 = img_to_base64(cnet_image)\n",
    "#     mask_base64 = img_to_base64(mask)\n",
    "#     headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "#     payload = {\n",
    "#         \"order_id\": order_id,\n",
    "#         \"c_net_base64\": c_net_base64,\n",
    "#         \"mask_base64\": mask_base64,\n",
    "#         \"num_inference_steps\": num_inference_steps,\n",
    "#         \"prompt_input\": prompt_input\n",
    "# }\n",
    "\n",
    "#     response = requests.post(url, json=payload, headers=headers)\n",
    "#     output_image_b64 = response.json()[\"output_image\"]\n",
    "#     output_image = Image.open(BytesIO(base64.b64decode(output_image_b64)))\n",
    "#     return output_image\n",
    "\n",
    "    (\n",
    "        prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds,\n",
    "    ) = pipe.encode_prompt(final_prompt, \"cuda\", True)\n",
    "\n",
    "    for image in pipe(\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,\n",
    "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "        image=cnet_image,\n",
    "        num_inference_steps=num_inference_steps\n",
    "    ):\n",
    "        cnet_image, image = cnet_image, image\n",
    "\n",
    "    image = image.convert(\"RGBA\")\n",
    "    mask = mask.resize(image.size)\n",
    "    cnet_image.paste(image, (0, 0), mask)\n",
    "\n",
    "    return background , cnet_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Image : (1280, 854)\n",
      "Original Size : (1280, 854)\n",
      "Aspect Ratio : 0.6671875\n",
      "New Width : 1280\n",
      "New Height : 854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define input parameters here\n",
    "    image_path = \"/home/ubuntu/Varad/diffusion-extender/assets/beach.jpg\"  # Change this to your input image path\n",
    "    width = 2048\n",
    "    height = 2048\n",
    "    overlap_width = 42\n",
    "    num_inference_steps = 10\n",
    "    resize_option = \"Full\"  # Options: \"Full\", \"1/2\", \"1/3\", \"1/4\", \"Custom\"\n",
    "    custom_resize_size = 512\n",
    "    prompt_input = \"High quality, 4k\"  # Optional prompt\n",
    "    alignment = \"Middle\" \n",
    "\n",
    "    background ,result_image = infer(image_path, width, height, overlap_width, num_inference_steps, resize_option, custom_resize_size, prompt_input, alignment)\n",
    "\n",
    "\n",
    "    # background.save(\"background_image.jpg\")\n",
    "    # result_image.save(\"result_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://164.52.212.87:8000/generate-image/\"\n",
    "\n",
    "margin_x = 0 \n",
    "margin_y = 0\n",
    "height = 640\n",
    "width = 640 \n",
    "image_url = \"https://ai-image-editor-wasabi-bucket.apyhi.com/assets/AI_PHOTO_UNCROP/image_extender_new_6.webp\"\n",
    "num_inference_steps = 10\n",
    "overlap_width = 42\n",
    "resize_option = \"Full\"\n",
    "\n",
    "payload = {         \n",
    "\"order_id\" : \"1234567890\",\n",
    "\"image_url\" : image_url,\n",
    "\"width\" : width,\n",
    "\"height\" : height,\n",
    "\"overlap_width\" : overlap_width,\n",
    "\"num_inference_steps\" : num_inference_steps,\n",
    "\"resize_option\" : resize_option,\n",
    "# \"prompt_input\" : prompt_input,\n",
    "\"margin_x\" : margin_x,\n",
    "\"margin_y\" : margin_y,\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(url, json=payload)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(response.json()['status_code'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "output_image_b64 = response.json()[\"output_image\"]\n",
    "output_image = Image.open(BytesIO(base64.b64decode(output_image_b64)))\n",
    "# output_image #.save(\"output_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
